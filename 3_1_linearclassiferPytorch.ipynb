{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9dspckUp6kI"
   },
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTfiVDyMp6kJ"
   },
   "source": [
    "In this section, you are going to download the data from IBM object storage using <b>wget</b>, then unzip them.  <b>wget</b> is a command the retrieves content from web servers, in this case its a zip file. Locally we store the data in the directory  <b>/resources/data</b> . The <b>-p</b> creates the entire directory tree up to the given directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay8dPOGNp6kJ"
   },
   "source": [
    "First, we download the file that contains the images, if you dint do this in your first lab uncomment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qrOJQUWp6kK",
    "outputId": "844e03ff-df61-49f9-8468-16e5b5bc5488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-23 06:18:38--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 245259777 (234M) [application/zip]\n",
      "Saving to: ‘/resources/data/concrete_crack_images_for_classification.zip’\n",
      "\n",
      "concrete_crack_imag 100%[===================>] 233.90M  30.7MB/s    in 7.9s    \n",
      "\n",
      "2021-08-23 06:18:47 (29.7 MB/s) - ‘/resources/data/concrete_crack_images_for_classification.zip’ saved [245259777/245259777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip -P /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLrbMDLgp6kK"
   },
   "source": [
    "We then unzip the file, this ma take a while:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Plp6g4Iup6kK"
   },
   "outputs": [],
   "source": [
    "!unzip -q  /resources/data/concrete_crack_images_for_classification.zip -d  /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYFmEBpTp6kL"
   },
   "source": [
    "We then download the files that contain the negative images:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNGGLS4Pp6kL"
   },
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nV-ewgYp6kL"
   },
   "source": [
    "The following are the libraries we are going to use for this lab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_oUuTqJOp6kL"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9hxNY3ap6kL"
   },
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQBCXio4p6kL"
   },
   "source": [
    "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uQnygLcGp6kL"
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"/resources/data\"\n",
    "        positive=\"Positive\"\n",
    "        negative=\"Negative\"\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
    "        positive_files.sort()\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
    "        negative_files.sort()\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:30000]\n",
    "            self.Y=self.Y[0:30000]\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)    \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        image=Image.open(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "          \n",
    "        \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tue-nPWWp6kM"
   },
   "source": [
    "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msM-sAvNp6kM"
   },
   "source": [
    "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lhqq5Q5Qp6kM"
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# transforms.ToTensor()\n",
    "#transforms.Normalize(mean, std)\n",
    "#transforms.Compose([])\n",
    "\n",
    "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOyf5WpFp6kM"
   },
   "source": [
    "Create object for the training data  <code>dataset_train</code> and validation <code>dataset_val</code>. Use the transform object to convert the images to tensors using the transform object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lzW-JNz_p6kN"
   },
   "outputs": [],
   "source": [
    "dataset_train=Dataset(transform=transform,train=True)\n",
    "dataset_val=Dataset(transform=transform,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktYJSut7p6kN"
   },
   "source": [
    "We  can find the shape of the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRs8Gfvsp6kN",
    "outputId": "b7e67533-7506-4e43-c3ff-c268e0e44815"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0zZEGDRp6kN"
   },
   "source": [
    "We see that it's a color image with three channels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHNY7xgwp6kN",
    "outputId": "6894ea20-b5ad-4042-9bd9-2e00150810ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154587"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_image=3*227*227\n",
    "size_of_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29Qutf1mp6kO",
    "outputId": "7f90f488-a295-4a27-a16d-5600889863ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa157261fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBUQmSBFp6kO"
   },
   "source": [
    "<b>Custom Module:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8mEdCDv2p6kO"
   },
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(SoftMax,self).__init__()\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjIy_zF0p6kO"
   },
   "source": [
    "<b>Model Object:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9W1Ax9J2p6kO"
   },
   "outputs": [],
   "source": [
    "model = SoftMax(size_of_image,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNpsxBJOp6kO",
    "outputId": "0b325d1a-ace4-4542-89da-b554a073c06f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftMax(\n",
       "  (linear): Linear(in_features=154587, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlRARl5Pp6kP"
   },
   "source": [
    "<b>Optimizer:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLN7t5Qyp6kP",
    "outputId": "33c00ad0-a82c-4768-8c98-d70c3798392d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.1\n",
       "    momentum: 0.1\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.1\n",
    "momentum = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3i2Cvwyp6kP"
   },
   "source": [
    "<b>Criterion:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mqY40zPCp6kP"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztQ4CqJmp6kP"
   },
   "source": [
    "<b>Data Loader Training and Validation:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7_tiYIr0p6kP"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, batch_size=1000)\n",
    "validation_loader = DataLoader(dataset=dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmkv8PHHp6kP",
    "outputId": "a8a62f1b-118a-4384-c8f6-80019dfb0069"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnN-MRsTp6kP"
   },
   "source": [
    "<b>Train Model with 5 epochs, should take 35 minutes: </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UHu7oVTp6kP",
    "outputId": "af1fa8cc-4147-4a73-b0a6-75f72ed6bf08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 73483.194, Accuracy: 0.620\n",
      "Epoch 2/5, Loss: 58244.811, Accuracy: 0.554\n",
      "Epoch 3/5, Loss: 44534.600, Accuracy: 0.570\n",
      "Epoch 4/5, Loss: 25873.868, Accuracy: 0.569\n",
      "Epoch 5/5, Loss: 31618.048, Accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test = len(dataset_val)\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model(x.view(-1,size_of_image))\n",
    "        loss = criterion(z,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost += loss.item()\n",
    "    correct=0\n",
    "    # perform a prediction on the validation data\n",
    "    model.eval()\n",
    "    for x_val, y_val in validation_loader:\n",
    "        z = model(x_val.view(-1, size_of_image))\n",
    "        _, yhat = torch.max(z.data,1)\n",
    "        correct += (yhat == y_val).sum().item()\n",
    "    accuracy = correct/N_test\n",
    "    accuracy_list.append(accuracy)\n",
    "    cost_list.append(cost)\n",
    "\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1,n_epochs, cost, accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3.1_linearclassiferPytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
